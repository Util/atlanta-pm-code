#summary The basic workflow of creating tests for this project.
#labels Phase-Implementation


*This is a work in progress, right now I am just trying to get ideas down and will come back around and clean up the writing...*


= Introduction =

An overview of the work flow for working on this project.


= Details =

*Pull the latest code from the repository*

This is covered on the Source tab.

* Code Coverage *

Make sure to have Devel::Cover installed. This perl module provide tools for checking the code coverage.

To install:
{{{
[root@laptop atlanta-pm-code]$ cpan Devel::Cover
}}}

Once installed run the command to generate the test coverage information, 'cover -test'

{{{
[woody@laptop atlanta-pm-code]$ cover -test
Deleting database /home/woody/atlanta-pm-code/cover_db
cover: running make test OPTIMIZE=-O0\ -fprofile-arcs\ -ftest-coverage OTHERLDFLAGS=-fprofile-arcs\ -ftest-coverage
cp lib/Tree/DAG_Node.pm blib/lib/Tree/DAG_Node.pm
PERL_DL_NONLAZY=1 /usr/bin/perl "-MExtUtils::Command::MM" "-e" "test_harness(0, 'blib/lib', 'blib/arch')" t/*.t
t/00_about_verbose.t ......... ok   
t/01_old_junk.t .............. ok   
t/02_add_daughter_wrapper.t .. ok   
t/copy_at_and_under.t ........ ok   
t/copy_tree.t ................ ok   
t/mother.t ................... ok   
t/new.t ...................... ok   
All tests successful.
Files=7, Tests=34, 22 wallclock secs ( 0.05 usr  0.07 sys +  9.44 cusr 10.24 csys = 19.80 CPU)
Result: PASS
Reading database from /home/woody/atlanta-pm-code/cover_db


---------------------------- ------ ------ ------ ------ ------ ------ ------
File                           stmt   bran   cond    sub    pod   time  total
---------------------------- ------ ------ ------ ------ ------ ------ ------
...l/5.10.0/Devel/Symdump.pm    0.0    0.0    0.0    0.0  100.0    n/a    2.2
...rl/5.10.0/Pod/Coverage.pm    0.0    0.0    0.0    0.0  100.0    n/a    2.3
.../Coverage/CountParents.pm    0.0    0.0    n/a    0.0    n/a    n/a    0.0
blib/lib/Tree/DAG_Node.pm      22.2   15.5    9.7   30.8   92.1  100.0   23.0
Total                          15.1   11.7    6.2   23.3   93.3  100.0   16.9
---------------------------- ------ ------ ------ ------ ------ ------ ------


Writing HTML output to /home/woody/atlanta-pm-code/cover_db/coverage.html ...
done.
}}}

Using your browser of choice, load the page "/home/$USERNAME/atlanta-pm-code/cover_db/coverage.html" to examine the code coverage. 

Click on the link: 'blib/lib/Tree/DAG_Node.pm'

At the top you can see the percentage of code coverage.

http://atlanta-pm-code.googlecode.com/svn/wiki/Workflow.attach/headers.png

I believe that the main focus is on the first three columns.
line - Line number of the code
stmt (Statement coverage) - How many times that statement was tested.
bran (Branch coverage) - Shows if the tests covered both choices of a brnching statement. 
cond (Condition coverage) - Shows if the tests covered all possibilities of a logical condition.
sub  (Subroutine coverage) - Shows how much of the subroutine was tested. 

Here is an example of an untested subroutine:
http://atlanta-pm-code.googlecode.com/svn/wiki/Workflow.attach/coverage.png


*Create or take ownership of an issue*

If you see an existing issue you want to take ownership of do so or look at the code coverage and find out which subroutines have not been tested yet and create a new Issue stating that subroutine has not been tested.

For the example above we will create a new issue for subroutines 'add_daughters_left' & 'add_daughter_left' not being tested.

http://atlanta-pm-code.googlecode.com/svn/wiki/Workflow.attach/add_issue.png


*Write the tests for the subroutine*

The tests exist under the "t/" directory. The current standard is to use the subroutine name as the filename of the test with the ".t" extension.

{{{
vi t/add_daughters_left.t

vi t/add_daughter_left.t
}}}


*Test your test*

I use the following line when testing tests:
prove -v -Ilib t/test_filename.t


*Add your test to the repository*

svn add t/test_filename.t
svn commit -m "Test created for Issue #10" t/test_filename.t


*Update and Close the Issue*

Update the issue with the revision number that has the test that was added and update the issues status as "Fixed".


*Wash, rinse, repeat*
